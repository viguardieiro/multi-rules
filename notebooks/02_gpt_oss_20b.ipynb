{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Multi-Subset InstABoost with GPT-OSS-20B (Reasoning Model)\n",
    "\n",
    "This notebook demonstrates InstABoost with OpenAI's GPT-OSS-20B, a reasoning model.\n",
    "\n",
    "We'll:\n",
    "1. Load the reasoning model\n",
    "2. Create prompts using the model's chat template\n",
    "3. Apply boosting to instruction tokens\n",
    "4. Compare outputs with and without boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu128\n",
      "CUDA available: True\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "GPU Memory: 85.1 GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from src.boost_config import TokenSubset, BoostConfig\n",
    "from src.token_utils import create_token_subset_from_substring\n",
    "from src.attention_hook import register_boost_hooks, unregister_boost_hooks, update_bias_mask\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading openai/gpt-oss-20b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c2b63b53cf43cdbb99b5c7e489cb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n",
      "Number of layers: 24\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai/gpt-oss-20b\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n",
    "\n",
    "GPT-OSS-20B is a reasoning model that outputs in channels: `analysis` (reasoning) and `final` (answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def parse_gpt_oss_output(generated_text: str) -> dict:\n",
    "    \"\"\"Parse GPT-OSS-20B output into reasoning and final answer.\"\"\"\n",
    "    result = {'reasoning': None, 'final': None, 'raw': generated_text}\n",
    "    \n",
    "    # Extract analysis/reasoning channel\n",
    "    analysis_match = re.search(\n",
    "        r'<\\|channel\\|>(?:analysis|commentary)<\\|message\\|>(.*?)(?:<\\|end\\|>|<\\|channel\\|>|$)', \n",
    "        generated_text, re.DOTALL\n",
    "    )\n",
    "    if analysis_match:\n",
    "        result['reasoning'] = analysis_match.group(1).strip()\n",
    "    \n",
    "    # Extract final answer channel\n",
    "    final_match = re.search(\n",
    "        r'<\\|channel\\|>final<\\|message\\|>(.*?)(?:<\\|end\\|>|<\\|return\\|>|<\\|channel\\|>|$)', \n",
    "        generated_text, re.DOTALL\n",
    "    )\n",
    "    if final_match:\n",
    "        result['final'] = final_match.group(1).strip()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def display_parsed_output(parsed: dict, max_reasoning_chars: int = 300):\n",
    "    \"\"\"Display parsed output with truncated reasoning.\"\"\"\n",
    "    reasoning = parsed['reasoning'] or \"(no reasoning found)\"\n",
    "    final = parsed['final'] or \"(no final answer)\"\n",
    "    \n",
    "    if len(reasoning) > max_reasoning_chars:\n",
    "        reasoning = reasoning[:max_reasoning_chars] + \"...\"\n",
    "    \n",
    "    print(f\"Reasoning: {reasoning}\")\n",
    "    print(f\"\\nFinal Answer: {final}\")\n",
    "\n",
    "def check_word_count(instruction: str, answer: str) -> tuple[bool, str]:\n",
    "    \"\"\"Check if answer follows word count instruction.\"\"\"\n",
    "    if answer is None:\n",
    "        return False, \"No final answer generated\"\n",
    "    \n",
    "    match = re.search(r'(\\d+)\\s*words?', instruction.lower())\n",
    "    if match:\n",
    "        target = int(match.group(1))\n",
    "        words = answer.split()\n",
    "        return len(words) == target, f\"Expected {target} words, got {len(words)}: {words}\"\n",
    "    return True, \"No word count constraint found\"\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Prepare Test Input\n",
    "\n",
    "We use a word count constraint - the model often struggles with these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Answer in exactly 3 words: What is the capital of France?\n",
      "Token count: 81\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Answer in exactly 3 words: What is the capital of France?\"\n",
    "instruction = \"Answer in exactly 3 words\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "input_length = input_ids.shape[1]\n",
    "\n",
    "print(f\"Prompt: {user_message}\")\n",
    "print(f\"Token count: {input_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Generate Baseline (No Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating baseline (no boosting)...\n",
      "Generated 600 tokens (HIT LIMIT)\n",
      "Reasoning: The user asks: \"Answer in exactly 3 words: What is the capital of France?\" They want a 3-word answer. The capital of France is Paris. But we need exactly 3 words. So we could say \"Paris, France's capital.\" That's 4 words? Let's count: \"Paris,\" (1) \"France's\" (2) \"capital.\" (3). Actually \"Paris, Fran...\n",
      "\n",
      "Final Answer: (no final answer)\n",
      "\n",
      "Instruction followed: NO - No final answer generated\n"
     ]
    }
   ],
   "source": [
    "MAX_NEW_TOKENS = 600\n",
    "\n",
    "print(\"Generating baseline (no boosting)...\")\n",
    "with torch.no_grad():\n",
    "    output_baseline = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "num_generated = output_baseline[0].shape[0] - input_length\n",
    "hit_limit = num_generated >= MAX_NEW_TOKENS\n",
    "\n",
    "generated_text = tokenizer.decode(output_baseline[0][input_length:], skip_special_tokens=False)\n",
    "parsed_baseline = parse_gpt_oss_output(generated_text)\n",
    "\n",
    "print(f\"Generated {num_generated} tokens {'(HIT LIMIT)' if hit_limit else ''}\")\n",
    "display_parsed_output(parsed_baseline)\n",
    "\n",
    "baseline_passed, baseline_reason = check_word_count(instruction, parsed_baseline['final'])\n",
    "print(f\"\\nInstruction followed: {'YES' if baseline_passed else 'NO'} - {baseline_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Generate With Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boosting tokens: ['Answer', ' in', ' exactly', ' ', '3', ' words']\n",
      "Bias: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Create boost configuration\n",
    "tokens = tokenizer.encode(formatted_prompt, add_special_tokens=False)\n",
    "instruction_subset = create_token_subset_from_substring(\n",
    "    name=\"instruction\",\n",
    "    text=formatted_prompt,\n",
    "    substring=instruction,\n",
    "    tokenizer=tokenizer,\n",
    "    bias=2.0\n",
    ")\n",
    "config = BoostConfig(subsets=[instruction_subset])\n",
    "\n",
    "print(f\"Boosting tokens: {[tokenizer.decode([tokens[i]]) for i in instruction_subset.indices]}\")\n",
    "print(f\"Bias: {instruction_subset.bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with boosting...\n",
      "Generated 301 tokens \n",
      "Reasoning: The user asks: \"Answer in exactly 3 words: What is the capital of France?\" They want exactly 3 words. The capital of France is Paris. But we need exactly 3 words. So we could say \"Paris, France's capital.\" That's 4 words? Let's count: \"Paris,\" (1) \"France's\" (2) \"capital.\" (3). Actually \"Paris,\" is ...\n",
      "\n",
      "Final Answer: Paris, France's capital.\n",
      "\n",
      "Instruction followed: YES - Expected 3 words, got 3: ['Paris,', \"France's\", 'capital.']\n"
     ]
    }
   ],
   "source": [
    "# Register hooks and generate\n",
    "handle = register_boost_hooks(model, config, input_length=input_length)\n",
    "update_bias_mask(handle, seq_length=input_length, device=device)\n",
    "\n",
    "print(\"Generating with boosting...\")\n",
    "with torch.no_grad():\n",
    "    output_boosted = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "unregister_boost_hooks(handle)\n",
    "\n",
    "num_generated = output_boosted[0].shape[0] - input_length\n",
    "hit_limit = num_generated >= MAX_NEW_TOKENS\n",
    "\n",
    "boosted_text = tokenizer.decode(output_boosted[0][input_length:], skip_special_tokens=False)\n",
    "parsed_boosted = parse_gpt_oss_output(boosted_text)\n",
    "\n",
    "print(f\"Generated {num_generated} tokens {'(HIT LIMIT)' if hit_limit else ''}\")\n",
    "display_parsed_output(parsed_boosted)\n",
    "\n",
    "boosted_passed, boosted_reason = check_word_count(instruction, parsed_boosted['final'])\n",
    "print(f\"\\nInstruction followed: {'YES' if boosted_passed else 'NO'} - {boosted_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARISON\n",
      "======================================================================\n",
      "\n",
      "Instruction: \"Answer in exactly 3 words\"\n",
      "\n",
      "Method          Final Answer                   Followed? \n",
      "----------------------------------------------------------------------\n",
      "Baseline        (none)                         NO        \n",
      "Boosted         Paris, France's capital.       YES       \n",
      "\n",
      "======================================================================\n",
      "VERDICT: Boosting HELPED\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nInstruction: \\\"{instruction}\\\"\")\n",
    "print(f\"\\n{'Method':<15} {'Final Answer':<30} {'Followed?':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "baseline_answer = parsed_baseline['final'] or '(none)'\n",
    "boosted_answer = parsed_boosted['final'] or '(none)'\n",
    "\n",
    "print(f\"{'Baseline':<15} {baseline_answer:<30} {'YES' if baseline_passed else 'NO':<10}\")\n",
    "print(f\"{'Boosted':<15} {boosted_answer:<30} {'YES' if boosted_passed else 'NO':<10}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "if boosted_passed and not baseline_passed:\n",
    "    print(\"VERDICT: Boosting HELPED\")\n",
    "elif boosted_passed and baseline_passed:\n",
    "    print(\"VERDICT: Both followed the instruction\")\n",
    "elif not boosted_passed and baseline_passed:\n",
    "    print(\"VERDICT: Boosting HURT\")\n",
    "else:\n",
    "    print(\"VERDICT: Neither followed the instruction\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Test Different Bias Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different bias values...\n",
      "\n",
      "Bias     Tokens     Passed?    Answer                        \n",
      "------------------------------------------------------------\n",
      "0.0      600        NO         (none)                        \n",
      "0.5      600        NO         (none)                        \n",
      "1.0      132        YES        Paris is capital.             \n",
      "2.0      301        YES        Paris, France's capital.      \n",
      "3.0      242        YES        Paris is capital              \n",
      "5.0      377        YES        Paris is capital              \n"
     ]
    }
   ],
   "source": [
    "bias_values = [0.0, 0.5, 1.0, 2.0, 3.0, 5.0]\n",
    "results = {}\n",
    "\n",
    "print(\"Testing different bias values...\\n\")\n",
    "\n",
    "for bias in bias_values:\n",
    "    if bias == 0.0:\n",
    "        # No boost\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids, max_new_tokens=MAX_NEW_TOKENS, do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "    else:\n",
    "        subset = create_token_subset_from_substring(\n",
    "            \"instruction\", formatted_prompt, instruction, tokenizer, bias\n",
    "        )\n",
    "        cfg = BoostConfig(subsets=[subset])\n",
    "        hdl = register_boost_hooks(model, cfg, input_length=input_length)\n",
    "        update_bias_mask(hdl, seq_length=input_length, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids, max_new_tokens=MAX_NEW_TOKENS, do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        unregister_boost_hooks(hdl)\n",
    "    \n",
    "    num_tok = output[0].shape[0] - input_length\n",
    "    gen_text = tokenizer.decode(output[0][input_length:], skip_special_tokens=False)\n",
    "    parsed = parse_gpt_oss_output(gen_text)\n",
    "    passed, reason = check_word_count(instruction, parsed['final'])\n",
    "    \n",
    "    results[bias] = {'answer': parsed['final'], 'passed': passed, 'tokens': num_tok}\n",
    "\n",
    "# Summary\n",
    "print(f\"{'Bias':<8} {'Tokens':<10} {'Passed?':<10} {'Answer':<30}\")\n",
    "print(\"-\" * 60)\n",
    "for bias, data in results.items():\n",
    "    answer = data['answer'] or '(none)'\n",
    "    answer = answer[:27] + '...' if len(answer) > 30 else answer\n",
    "    print(f\"{bias:<8} {data['tokens']:<10} {'YES' if data['passed'] else 'NO':<10} {answer:<30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- **Optimal bias**: 1.0-2.0 works well for GPT-OSS-20B\n",
    "- **Too high bias** (5.0+): Can cause repetition in reasoning\n",
    "- **Word count constraints**: The model often gets stuck counting words in reasoning - boosting can help it commit to an answer faster\n",
    "\n",
    "### For More Tests\n",
    "\n",
    "Run the standalone test script:\n",
    "```bash\n",
    "python ../scripts/test_baseline_instructions.py\n",
    "```\n",
    "\n",
    "Or from this notebook:\n",
    "```python\n",
    "%run ../scripts/test_baseline_instructions.py\n",
    "results = run_tests(model, tokenizer, device)\n",
    "print_summary(results)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
