{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Multi-Subset InstABoost with GPT-OSS-20B (Reasoning Model)\n",
    "\n",
    "This notebook demonstrates InstABoost with OpenAI's GPT-OSS-20B, a reasoning model.\n",
    "\n",
    "We'll:\n",
    "1. Load the reasoning model\n",
    "2. Create prompts using the model's chat template\n",
    "3. Apply boosting to instruction tokens\n",
    "4. Compare outputs with and without boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from src.boost_config import TokenSubset, BoostConfig\n",
    "from src.token_utils import create_token_subset_from_substring\n",
    "from src.attention_hook import register_boost_hooks, unregister_boost_hooks, update_bias_mask\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load Model and Tokenizer\n",
    "\n",
    "Loading GPT-OSS-20B in bfloat16 for efficient memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/gpt-oss-20b\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Model type: {model.config.model_type}\")\n",
    "print(f\"Number of layers: {model.config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Prepare Input with Chat Template\n",
    "\n",
    "GPT-OSS-20B uses a specific chat template with channels for reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple instruction-following task\n",
    "user_message = \"Answer in exactly 3 words: What is the capital of France?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Formatted prompt:\")\n",
    "print(formatted_prompt)\n",
    "print(f\"\\nPrompt length: {len(formatted_prompt)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and examine\n",
    "input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "input_length = input_ids.shape[1]\n",
    "\n",
    "print(f\"Token count: {input_length}\")\n",
    "print(f\"\\nTokens (showing user message portion):\")\n",
    "\n",
    "# Find where the user message starts\n",
    "tokens = tokenizer.encode(formatted_prompt, add_special_tokens=False)\n",
    "for i, token_id in enumerate(tokens):\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    # Highlight the user message tokens\n",
    "    if user_message[:10] in tokenizer.decode(tokens[max(0,i-5):i+5]):\n",
    "        print(f\"  {i}: '{token_text}' (id={token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Generate Without Boosting (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating without boosting...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_baseline = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,  # Deterministic for comparison\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "baseline_text = tokenizer.decode(output_baseline[0], skip_special_tokens=False)\n",
    "print(f\"\\nBaseline output:\")\n",
    "print(baseline_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract just the generated part (after the prompt)\n",
    "generated_tokens = output_baseline[0][input_length:]\n",
    "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=False)\n",
    "print(\"Generated (after prompt):\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Create Boost Configuration\n",
    "\n",
    "We'll boost attention to the instruction part: \"Answer in exactly 3 words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Find the instruction substring in the formatted prompt\ninstruction = \"Answer in exactly 3 words\"\n\n# Create token subset for the instruction\n# NOTE: bias=2.0 is optimal for GPT-OSS-20B. Higher values cause repetition.\ninstruction_subset = create_token_subset_from_substring(\n    name=\"instruction\",\n    text=formatted_prompt,\n    substring=instruction,\n    tokenizer=tokenizer,\n    bias=2.0  # Optimal boost for this model\n)\n\nprint(f\"Instruction subset: {instruction_subset}\")\nprint(f\"Token indices: {instruction_subset.indices}\")\n\n# Show which tokens are being boosted\nprint(f\"\\nBoosted tokens:\")\nfor idx in instruction_subset.indices:\n    token_text = tokenizer.decode([tokens[idx]])\n    print(f\"  {idx}: '{token_text}'\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boost configuration\n",
    "config = BoostConfig(subsets=[instruction_subset])\n",
    "print(f\"Boost config: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. Generate With Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hooks\n",
    "print(\"Registering boost hooks...\")\n",
    "handle = register_boost_hooks(model, config, input_length=input_length)\n",
    "\n",
    "# Update bias mask with actual sequence length\n",
    "update_bias_mask(handle, seq_length=input_length, device=device)\n",
    "\n",
    "print(f\"Bias mask shape: {handle.bias_mask.shape}\")\n",
    "print(f\"Non-zero bias positions: {(handle.bias_mask != 0).sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating with boosting...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_boosted = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,  # Deterministic for comparison\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "boosted_text = tokenizer.decode(output_boosted[0], skip_special_tokens=False)\n",
    "print(f\"\\nBoosted output:\")\n",
    "print(boosted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract just the generated part\n",
    "boosted_generated_tokens = output_boosted[0][input_length:]\n",
    "boosted_generated_text = tokenizer.decode(boosted_generated_tokens, skip_special_tokens=False)\n",
    "print(\"Generated with boost (after prompt):\")\n",
    "print(boosted_generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up hooks\n",
    "unregister_boost_hooks(handle)\n",
    "print(\"Hooks unregistered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 6. Compare Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"COMPARISON\")\nprint(\"=\" * 80)\nprint(f\"\\nInstruction: {instruction}\")\nprint(f\"Boost bias: {instruction_subset.bias}\")\nprint(f\"\\n{'-'*40}\")\nprint(\"BASELINE (no boosting):\")\nprint(generated_text)\nprint(f\"\\n{'-'*40}\")\nprint(\"BOOSTED (instruction bias=2.0):\")\nprint(boosted_generated_text)\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. Test with Different Bias Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# Test different bias values to find optimal range\n# Based on experiments: 1.0-2.0 is good, 5.0+ causes problems\nbias_values = [0.5, 1.0, 2.0, 3.0, 5.0]\n\nprint(\"Testing different bias values...\")\nprint(\"(Watch for repetition at higher values!)\\n\")\nresults = {}\n\nfor bias in bias_values:\n    # Create config with new bias\n    subset = create_token_subset_from_substring(\n        \"instruction\", formatted_prompt, instruction, tokenizer, bias\n    )\n    config = BoostConfig(subsets=[subset])\n    \n    # Register and generate\n    handle = register_boost_hooks(model, config, input_length=input_length)\n    update_bias_mask(handle, seq_length=input_length, device=device)\n    \n    with torch.no_grad():\n        output = model.generate(\n            input_ids,\n            max_new_tokens=150,\n            do_sample=False,\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n    \n    generated = tokenizer.decode(output[0][input_length:], skip_special_tokens=False)\n    results[bias] = generated\n    \n    unregister_boost_hooks(handle)\n    \n    print(f\"Bias={bias}:\")\n    print(f\"{generated[:300]}...\" if len(generated) > 300 else generated)\n    print(\"-\" * 80)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 8. Test with a Different Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a different instruction that's easier to verify\n",
    "user_message2 = \"Respond only in Spanish: What is 2 + 2?\"\n",
    "\n",
    "messages2 = [{\"role\": \"user\", \"content\": user_message2}]\n",
    "formatted_prompt2 = tokenizer.apply_chat_template(messages2, tokenize=False, add_generation_prompt=True)\n",
    "input_ids2 = tokenizer(formatted_prompt2, return_tensors=\"pt\").input_ids.to(device)\n",
    "input_length2 = input_ids2.shape[1]\n",
    "\n",
    "print(f\"New instruction: '{user_message2}'\")\n",
    "print(f\"Token count: {input_length2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "print(\"Baseline (no boost):\")\n",
    "with torch.no_grad():\n",
    "    out_base = model.generate(input_ids2, max_new_tokens=50, do_sample=False,\n",
    "                               pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(out_base[0][input_length2:], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "# Boosted - boost \"Respond only in Spanish\"\n# NOTE: bias=2.0 is optimal. Higher values (5.0+) can cause repetition loops!\ninstruction2 = \"Respond only in Spanish\"\nsubset2 = create_token_subset_from_substring(\n    \"spanish_instruction\", formatted_prompt2, instruction2, tokenizer, bias=2.0\n)\nconfig2 = BoostConfig(subsets=[subset2])\n\nhandle2 = register_boost_hooks(model, config2, input_length=input_length2)\nupdate_bias_mask(handle2, seq_length=input_length2, device=device)\n\nprint(f\"\\nBoosted (bias=2.0 on '{instruction2}'):\")\nwith torch.no_grad():\n    out_boost = model.generate(input_ids2, max_new_tokens=150, do_sample=False,\n                                pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id)\nprint(tokenizer.decode(out_boost[0][input_length2:], skip_special_tokens=False))\n\nunregister_boost_hooks(handle2)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrated:\n1. Loading GPT-OSS-20B (a reasoning model) with InstABoost\n2. Using the model's chat template for proper formatting\n3. Boosting attention to specific instruction tokens\n4. Comparing outputs with different bias values\n\n### Key Findings\n\n**Optimal Bias Values:**\n- `bias=2.0` - Sweet spot for this model. Helps focus on instructions without degradation.\n- `bias=5.0+` - Too strong! Causes repetition loops in the reasoning phase.\n- `bias=10.0` - Severe degradation, nonsensical output.\n\n**Effect on Reasoning Models:**\n- The boost affects both the `analysis` (reasoning) and `final` (output) phases\n- Boosted models often reach a final answer where baseline gets stuck in reasoning\n- The model's internal reasoning explicitly mentions the boosted instruction more prominently\n\n**Example Results:**\n- \"Respond only in Spanish: What is 2+2?\"\n  - Baseline: Gets stuck in reasoning, no final answer\n  - Boosted (2.0): Completes reasoning and outputs \"4\""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}