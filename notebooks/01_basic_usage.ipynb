{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage of Multi-Subset InstABoost\n",
    "\n",
    "This notebook demonstrates the basic usage of the multi-subset InstABoost implementation.\n",
    "\n",
    "We'll:\n",
    "1. Load a small transformer model (GPT-2)\n",
    "2. Create a simple instruction + question prompt\n",
    "3. Apply boosting to different parts with different bias values\n",
    "4. Compare outputs with and without boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path to import src modules\n",
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # Must be set before importing torch\n",
    "os.environ[\"USE_TORCH\"] = \"1\"  # Force Hugging Face to use PyTorch\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logging\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"  # Disable TensorFlow in transformers\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from src.boost_config import TokenSubset, BoostConfig\n",
    "from src.token_utils import create_token_subset_from_substring\n",
    "from src.attention_hook import register_boost_hooks, unregister_boost_hooks, update_bias_mask\n",
    "\n",
    "# Force CPU to avoid MPS hanging issues on macOS\n",
    "device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model and Tokenizer\n",
    "\n",
    "We'll start with GPT-2 small for quick testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2...\n",
      "Model loaded on cpu\n",
      "Model type: gpt2\n",
      "Number of layers: 12\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Use CPU device (defined in cell-1 to avoid MPS issues)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "print(f\"Model type: {model.config.model_type}\")\n",
    "print(f\"Number of layers: {model.config.n_layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Input\n",
    "\n",
    "Create a prompt with an instruction and a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: Instruction: Answer in French. Question: What is the capital of France?\n",
      "\n",
      "Tokenization:\n",
      "  0: 'Inst' (id=6310)\n",
      "  1: 'ruction' (id=2762)\n",
      "  2: ':' (id=25)\n",
      "  3: ' Answer' (id=23998)\n",
      "  4: ' in' (id=287)\n",
      "  5: ' French' (id=4141)\n",
      "  6: '.' (id=13)\n",
      "  7: ' Question' (id=18233)\n",
      "  8: ':' (id=25)\n",
      "  9: ' What' (id=1867)\n",
      "  10: ' is' (id=318)\n",
      "  11: ' the' (id=262)\n",
      "  12: ' capital' (id=3139)\n",
      "  13: ' of' (id=286)\n",
      "  14: ' France' (id=4881)\n",
      "  15: '?' (id=30)\n"
     ]
    }
   ],
   "source": [
    "# Create input text\n",
    "text = \"Instruction: Answer in French. Question: What is the capital of France?\"\n",
    "\n",
    "print(f\"Input text: {text}\")\n",
    "print(f\"\\nTokenization:\")\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "for i, token_id in enumerate(tokens):\n",
    "    token_text = tokenizer.decode([token_id])\n",
    "    print(f\"  {i}: '{token_text}' (id={token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Without Boosting (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating without boosting...\n",
      "\n",
      "Baseline output:\n",
      "Instruction: Answer in French. Question: What is the capital of France? Answer: Ville, Paris, France.\n",
      "\n",
      "Q. What is Paris? Answer: The capital of France.\n",
      "\n",
      "Q. What is\n"
     ]
    }
   ],
   "source": [
    "# Tokenize input\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "# Generate without boosting\n",
    "print(\"Generating without boosting...\")\n",
    "with torch.no_grad():\n",
    "    output_baseline = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[1] + 30,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "baseline_text = tokenizer.decode(output_baseline[0], skip_special_tokens=True)\n",
    "print(f\"\\nBaseline output:\\n{baseline_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Boost Configuration\n",
    "\n",
    "Define which parts of the input to boost and with what bias values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction subset: TokenSubset(name='instruction', indices=[0, 1, 2]..., bias=2.0)\n",
      "Question subset: TokenSubset(name='question', indices=[7, 8, 9]..., bias=1.0)\n",
      "\n",
      "Boost config: BoostConfig(2 subsets, all layers, all heads, combination='sum')\n"
     ]
    }
   ],
   "source": [
    "# Create token subsets from substrings\n",
    "instruction_subset = create_token_subset_from_substring(\n",
    "    name=\"instruction\",\n",
    "    text=text,\n",
    "    substring=\"Instruction: Answer in French.\",\n",
    "    tokenizer=tokenizer,\n",
    "    bias=2.0  # Strong boost for instruction\n",
    ")\n",
    "\n",
    "question_subset = create_token_subset_from_substring(\n",
    "    name=\"question\",\n",
    "    text=text,\n",
    "    substring=\"Question: What is the capital of France?\",\n",
    "    tokenizer=tokenizer,\n",
    "    bias=1.0  # Moderate boost for question\n",
    ")\n",
    "\n",
    "print(f\"Instruction subset: {instruction_subset}\")\n",
    "print(f\"Question subset: {question_subset}\")\n",
    "\n",
    "# Create boost configuration\n",
    "config = BoostConfig(subsets=[instruction_subset, question_subset])\n",
    "print(f\"\\nBoost config: {config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate With Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering boost hooks...\n",
      "Bias mask: tensor([2., 2., 2., 2., 2., 2., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "\n",
      "Generating with boosting...\n",
      "\n",
      "Boosted output:\n",
      "Instruction: Answer in French. Question: What is the capital of France? Answer: La City. Question: What is the capital of France? Answer: La Nationale. Question: What is the capital of France? Answer\n",
      "\n",
      "Hooks unregistered.\n"
     ]
    }
   ],
   "source": [
    "# Register hooks\n",
    "print(\"Registering boost hooks...\")\n",
    "handle = register_boost_hooks(model, config, input_length=input_ids.shape[1])\n",
    "\n",
    "# Update bias mask with actual sequence length\n",
    "update_bias_mask(handle, seq_length=input_ids.shape[1], device=device)\n",
    "\n",
    "print(f\"Bias mask: {handle.bias_mask}\")\n",
    "\n",
    "# Generate with boosting\n",
    "print(\"\\nGenerating with boosting...\")\n",
    "with torch.no_grad():\n",
    "    output_boosted = model.generate(\n",
    "        input_ids,\n",
    "        max_length=input_ids.shape[1] + 30,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "boosted_text = tokenizer.decode(output_boosted[0], skip_special_tokens=True)\n",
    "print(f\"\\nBoosted output:\\n{boosted_text}\")\n",
    "\n",
    "# Clean up hooks\n",
    "unregister_boost_hooks(handle)\n",
    "print(\"\\nHooks unregistered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Input:\n",
      "Instruction: Answer in French. Question: What is the capital of France?\n",
      "\n",
      "----------------------------Baseline (no boosting):-----------------------------\n",
      "Instruction: Answer in French. Question: What is the capital of France? Answer: Ville, Paris, France.\n",
      "\n",
      "Q. What is Paris? Answer: The capital of France.\n",
      "\n",
      "Q. What is\n",
      "\n",
      "--------------------Boosted (instruction=2.0, question=1.0):--------------------\n",
      "Instruction: Answer in French. Question: What is the capital of France? Answer: La City. Question: What is the capital of France? Answer: La Nationale. Question: What is the capital of France? Answer\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nInput:\\n{text}\")\n",
    "print(f\"\\n{'Baseline (no boosting):':-^80}\")\n",
    "print(baseline_text)\n",
    "print(f\"\\n{'Boosted (instruction=2.0, question=1.0):':-^80}\")\n",
    "print(boosted_text)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test with Different Bias Values\n",
    "\n",
    "Try different bias configurations to see their effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different bias values...\n",
      "\n",
      "Bias (inst=0.5, quest=0.5):\n",
      "Instruction: Answer in French. Question: What is the capital of France? Answer: France. Question: What is the name of the city in France? Answer: Paris. Question: What is the name of the country in\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Bias (inst=1.0, quest=0.5):\n",
      "Instruction: Answer in French. Question: What is the capital of France? Answer: The capital of France, and France, is France. Question: What is the name of the city in France as of August 31, 1862\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Bias (inst=3.0, quest=1.0):\n",
      "Instruction: Answer in French. Question: What is the capital of France? Answer: Fondation, Fondation, Fondation, Fondation, Fondation. Question: What is the capital of France\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Bias (inst=5.0, quest=2.0):\n",
      "Instruction: Answer in French. Question: What is the capital of France? Answer: The capital of France is the capital of the Republic of Brittany. Answer: The capital of France is the capital of the Republic of France.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test different bias values\n",
    "bias_values = [\n",
    "    (0.5, 0.5),   # Weak boosting\n",
    "    (1.0, 0.5),   # Moderate instruction boost\n",
    "    (3.0, 1.0),   # Strong instruction boost\n",
    "    (5.0, 2.0),   # Very strong boosting\n",
    "]\n",
    "\n",
    "print(\"Testing different bias values...\\n\")\n",
    "\n",
    "for inst_bias, quest_bias in bias_values:\n",
    "    # Create new config\n",
    "    inst_subset = create_token_subset_from_substring(\n",
    "        \"instruction\", text, \"Instruction: Answer in French.\",\n",
    "        tokenizer, inst_bias\n",
    "    )\n",
    "    quest_subset = create_token_subset_from_substring(\n",
    "        \"question\", text, \"Question: What is the capital of France?\",\n",
    "        tokenizer, quest_bias\n",
    "    )\n",
    "    config = BoostConfig(subsets=[inst_subset, quest_subset])\n",
    "\n",
    "    # Register hooks\n",
    "    handle = register_boost_hooks(model, config, input_length=input_ids.shape[1])\n",
    "    update_bias_mask(handle, seq_length=input_ids.shape[1], device=device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=input_ids.shape[1] + 30,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Clean up\n",
    "    unregister_boost_hooks(handle)\n",
    "\n",
    "    print(f\"Bias (inst={inst_bias}, quest={quest_bias}):\")\n",
    "    print(f\"{result}\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. Loading a model and tokenizer\n",
    "2. Creating token subsets from substrings\n",
    "3. Configuring multi-subset boosting\n",
    "4. Applying boosting during generation\n",
    "5. Comparing outputs with different bias values\n",
    "\n",
    "Next steps:\n",
    "- Try with larger models (Mistral-7B-Instruct)\n",
    "- Test with more complex prompts\n",
    "- Analyze attention patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
